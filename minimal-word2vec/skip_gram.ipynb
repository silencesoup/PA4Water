{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c1e451-39b6-4c9f-baf3-f1fb4a9aa3cc",
   "metadata": {},
   "source": [
    "# word2vec skip-gram model\n",
    "\n",
    "微型预料库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6751f5-dcaf-4a56-bf1a-f4fd7fc78b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f433460-675d-45de-93ec-39ef165a0fc6",
   "metadata": {},
   "source": [
    "创建词汇表\n",
    "\n",
    "大小写规范化，去除一些标点符号等，标记化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a400ff-f0b8-4e4a-a672-d8c126a5f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64971d44-f6ff-4267-a73c-a93c0b828907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'a', 'king'],\n",
       " ['she', 'is', 'a', 'queen'],\n",
       " ['he', 'is', 'a', 'man'],\n",
       " ['she', 'is', 'a', 'woman'],\n",
       " ['warsaw', 'is', 'poland', 'capital'],\n",
       " ['berlin', 'is', 'germany', 'capital'],\n",
       " ['paris', 'is', 'france', 'capital']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a881b-82f1-4889-b994-e48f5dfacd3c",
   "metadata": {},
   "source": [
    "接下来要建立word和index之间的mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b730bf-3017-40cd-ba89-b8fa1b49de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea53869-78e9-4813-bf38-ce1ba002c9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': 0,\n",
       " 'is': 1,\n",
       " 'a': 2,\n",
       " 'king': 3,\n",
       " 'she': 4,\n",
       " 'queen': 5,\n",
       " 'man': 6,\n",
       " 'woman': 7,\n",
       " 'warsaw': 8,\n",
       " 'poland': 9,\n",
       " 'capital': 10,\n",
       " 'berlin': 11,\n",
       " 'germany': 12,\n",
       " 'paris': 13,\n",
       " 'france': 14}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef21ac9-4086-4f07-b536-b10ceb0455f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'is',\n",
       " 'a',\n",
       " 'king',\n",
       " 'she',\n",
       " 'queen',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'warsaw',\n",
       " 'poland',\n",
       " 'capital',\n",
       " 'berlin',\n",
       " 'germany',\n",
       " 'paris',\n",
       " 'france']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02d0196-f338-48af-b79e-c1e1bd7835e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b07c67f-7622-4e0e-bb1f-6d2aff2945da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  3],\n",
       "       [ 2,  0],\n",
       "       [ 2,  1],\n",
       "       [ 2,  3],\n",
       "       [ 3,  1],\n",
       "       [ 3,  2],\n",
       "       [ 4,  1],\n",
       "       [ 4,  2],\n",
       "       [ 1,  4],\n",
       "       [ 1,  2],\n",
       "       [ 1,  5],\n",
       "       [ 2,  4],\n",
       "       [ 2,  1],\n",
       "       [ 2,  5],\n",
       "       [ 5,  1],\n",
       "       [ 5,  2],\n",
       "       [ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  6],\n",
       "       [ 2,  0],\n",
       "       [ 2,  1],\n",
       "       [ 2,  6],\n",
       "       [ 6,  1],\n",
       "       [ 6,  2],\n",
       "       [ 4,  1],\n",
       "       [ 4,  2],\n",
       "       [ 1,  4],\n",
       "       [ 1,  2],\n",
       "       [ 1,  7],\n",
       "       [ 2,  4],\n",
       "       [ 2,  1],\n",
       "       [ 2,  7],\n",
       "       [ 7,  1],\n",
       "       [ 7,  2],\n",
       "       [ 8,  1],\n",
       "       [ 8,  9],\n",
       "       [ 1,  8],\n",
       "       [ 1,  9],\n",
       "       [ 1, 10],\n",
       "       [ 9,  8],\n",
       "       [ 9,  1],\n",
       "       [ 9, 10],\n",
       "       [10,  1],\n",
       "       [10,  9],\n",
       "       [11,  1],\n",
       "       [11, 12],\n",
       "       [ 1, 11],\n",
       "       [ 1, 12],\n",
       "       [ 1, 10],\n",
       "       [12, 11],\n",
       "       [12,  1],\n",
       "       [12, 10],\n",
       "       [10,  1],\n",
       "       [10, 12],\n",
       "       [13,  1],\n",
       "       [13, 14],\n",
       "       [ 1, 13],\n",
       "       [ 1, 14],\n",
       "       [ 1, 10],\n",
       "       [14, 13],\n",
       "       [14,  1],\n",
       "       [14, 10],\n",
       "       [10,  1],\n",
       "       [10, 14]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb6a21-18a0-47a6-a7a9-cd657f814934",
   "metadata": {},
   "source": [
    "这个意思就是：\n",
    "\n",
    "“\n",
    "he is\n",
    "\n",
    "he a\n",
    "\n",
    "is he\n",
    "\n",
    "is a\n",
    "\n",
    "is king\n",
    "\n",
    "a he\n",
    "\n",
    "a is\n",
    "\n",
    "a king\n",
    "”\n",
    "\n",
    "![](1_uYiqfNrUIzkdMrmkBWGMPw.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4465c-ec52-465a-ae17-0ffc92b87bd4",
   "metadata": {},
   "source": [
    "定义目标函数是接下来关键一步\n",
    "\n",
    "skip-gram关心的是给定中心词，预测周围词出现的概率：$P(context|center;\\theta)$。\n",
    "\n",
    "通过遍历所有word/context对，来最大化这一概率：\n",
    "\n",
    "max $\\Pi _{center}\\Pi _{context}P(context|center;\\theta)$\n",
    "\n",
    "这个公式并不适合计算，所以需要一些转换，转换成求和公式。\n",
    "\n",
    "$min_\\theta\\ log \\Pi_{center}\\Pi_{context}P(context|center,\\theta)$\n",
    "\n",
    "$loss = -1/T \\Sigma _{center}\\Sigma _{context}log P(context|center, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fffa4e5-608d-499d-af23-4af7162ecd5a",
   "metadata": {},
   "source": [
    "接下来关键就是P(context|center)应该如何定义了。\n",
    "\n",
    "$P(context|center)=\\frac{exp(u^T_{context}\\ \\ \\ v_{center})}{\\Sigma _{\\omega \\in vocab}\\ \\ \\ \\ \\ exp(u^T_{\\omega}\\ v_{center})}$\n",
    "\n",
    "看起来很复杂，一步步分解下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85ee5b-b6e8-4cf5-afd2-1b9f595d1728",
   "metadata": {},
   "source": [
    "首先，整个分子分母的计算是一个softmax函数，控制概率0-1之间。\n",
    "\n",
    "分子exp括号里是 向量点积运算，就是余弦相似度，两个向量越接近，它们相乘值更大。\n",
    "\n",
    "分母里面，求和的意思是对词汇表中所有的向量求：给定一个center词，词汇表中所有词和它之间的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4a00b-f49f-4ae7-ac9b-6eebaa88ab1c",
   "metadata": {},
   "source": [
    "总的来说，就是：“对于语料库中每一个现有的 中心词-语境词 对，我们要计算它们的 \"相似度分数\"。然后用它除以每个理论上可能的语境的总和--以了解分数是相对较高还是较低。由于softmax被保证在0和1之间取值，它定义了一个有效的概率分布。”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tutorial)",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
